# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['DEFAULT_DEVICE', 'CapeModel']

# %% ../nbs/00_core.ipynb 4
from types import SimpleNamespace
from contextlib import nullcontext

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import DataLoader

from torchmetrics import Metric, Accuracy

from .utils import ifnone, to_device

# %% ../nbs/00_core.ipynb 5
DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# %% ../nbs/00_core.ipynb 7
if torch.cuda.is_available():
    mixed_precision = torch.autocast("cuda")

# %% ../nbs/00_core.ipynb 8
class CapeModel:
    def __init__(self, model, device=None, fp16=True, use_wandb=False):
        
        self.device = torch.device(ifnone(device, DEFAULT_DEVICE))
        self.model = model.to(self.device)
        self.fp16 = fp16 if self.device.type == "cuda" else False
        self.use_wandb = use_wandb
        
        self.config = SimpleNamespace(model_name=model.__class__, device=device)
        
    def _defaults(epochs=5, lr=2e-3, wd=0.01):
        self.optimizer = AdamW(self.model.parameters(), weight_decay=wd)
        self.loss_func = nn.CrossEntropyLoss()
        self.scheduler = OneCycleLR(self.optim, max_lr=lr, 
                                   steps_per_epoch=len(self.train_dataloader), 
                                   epochs=epochs)
        
        self.train_metrics = [Accuracy(task="multiclass").to(self.device), ]
        self.valid_metrics = [Accuracy(task="multiclass").to(self.device), ]
            
    def update_metrics(self, preds_b, labels, is_train=True, final=False):
        suffix = "train_" if is_train else "valid_"
        def metric_name(m): 
            return suffix + (type(m).__name__).lower()
        metrics = self.train_metrics if is_train else self.valid_metrics
        if not final:
            return {metric_name(m): m(preds_b, labels) for m in metrics}
        return {metric_name(m): m.compute for m in metrics}
    
    def compute_metrics(self):
        
        return {metric_name(m): m.compute for m in metrics}
    
    def log(self, d):
        if self.use_wandb:
            wandb.log(d)
    
    def compile(self, epochs=5, lr=2e-3, wd=0.01, optimizer=None, 
                loss_func=None, scheduler=None, train_metrics=None, valid_metrics=None):
        self.config.epochs = epochs
        self.config.lr = lr
        self.config.wd = wd
        
        # defaults
        self._defaults(epochs, lr, wd)
        self.optimizer = ifnone(optimizer, self.optimizer)
        self.loss_func = ifnone(loss_func, self.loss_func)
        self.scheduler = ifnone(scheduler, self.scheduler)
        self.train_metrics = ifnone(train_metrics, self.train_metrics)
        self.valid_metrics = ifnone(valid_metrics, self.valid_metrics)

        
    def train_step(self, preds, labels):
        loss = self.loss_func(preds_b, labels)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()
        
        # metrics
        metrics = self.update_metrics(preds_b, labels, is_train=True)
        metrics.update({"train_loss": loss.item(), "learning_rate": self.schedule.get_last_lr()[0]})
        self.log(metrics)
        return loss
    
    def valid_step(self, preds, labels):
        loss = self.loss_func(preds_b, labels)
        
        # metrics
        metrics = self.update_metrics(preds_b, labels, is_train=False)
        return loss
    
    def switch(self, is_train=True):
        if is_train:
            self.model.train()
            dl = self.train_dataloader
            ctx = torch.enable_grad()
        else:
            self.model.eval()
            dl = self.valid_dataloader
            ctx = torch.inference_mode()
        return dl, ctx
    
    def one_epoch(self, is_train=True):
        avg_loss = 0.
        dl, ctx = self.switch(is_train)
        pbar = progress_bar(dl, leave=False)
        preds = []
        for i, b in enumerate(pbar):
            with ctx and (mixed_precision if self.fp16 else nullcontext):
                images, labels = to_device(b, self.device)
                preds_b = self.model(images)         
                preds.append(preds_b)
                if is_train:
                    loss = self.train_step(preds_b, labels)
                else:
                    loss = self.valid_step(preds_b, labels)
                avg_loss += loss
            pbar.comment = f"loss={loss.item():2.3f}"
        return torch.cat(preds, dim=0), avg_loss.mean().item()
    
    def get_data_tensors():
        raise NotImplementedError()
    
    def get_model_preds(self, with_inputs=False):
        preds, loss = self.one_epoch(train=False)
        if with_inputs:
            images, labels = self.get_data_tensors()
            return images, labels, preds, loss
        else:
            return preds, loss
            
    def fit(self, train_dataloader, valid_dataloader=None):
        
        self.train_dataloader = train_dataloader
        self.valid_dataloader = valid_dataloader
            
        for epoch in progress_bar(range(self.config.epochs), total=self.config.epochs, leave=True):
            _  = self.one_epoch(train=True)
            
            self.log({"epoch":epoch})
                
            ## validation
            if valid_dataloader is not None:
                _, avg_loss = self.one_epoch(train=False)
                self.log({"val_loss": avg_loss, "val_acc": self.valid_acc.compute()})
                
            self.reset_metrics()
        if use_wandb:
            if log_preds:
                print("Logging model predictions on validation data")
                preds, _ = self.get_model_preds()
                self.preds_logger.log(preds=preds)
            wandb.finish()

# %% ../nbs/00_core.ipynb 9
class CapeModel:
    def __init__(self, data_path=".", tfms=tfms, model_name="convnext_nano", device="cuda", bs=256):
        
        self.device = device
        self.config = SimpleNamespace(model_name=model_name, device=device)
        
        self.model = timm.create_model(model_name, pretrained=False, num_classes=10, in_chans=1).to(device)
        
        self.train_ds = tv.datasets.FashionMNIST(data_path, download=True, 
                                                 transform=tfms["train"])
        self.valid_ds = tv.datasets.FashionMNIST(data_path, download=True, train=False, 
                                                 transform=tfms["valid"])
        
        self.train_acc = Accuracy(task="multiclass", num_classes=10).to(device)
        self.valid_acc = Accuracy(task="multiclass", num_classes=10).to(device)
        
        self.do_validation = True
        
        self.dataloaders(bs=bs)
        
        # get validation daat reference
        self.preds_logger = LogPreds(ds=self.valid_ds) 
    
    def dataloaders(self, bs=128, num_workers=8):
        self.config.bs = bs
        self.num_workers = num_workers
        self.train_dataloader = DataLoader(self.train_ds, batch_size=bs, shuffle=True, 
                                   pin_memory=True, num_workers=num_workers)
        self.valid_dataloader = DataLoader(self.valid_ds, batch_size=bs*2, shuffle=False, 
                                   num_workers=num_workers)

    def compile(self, epochs=5, lr=2e-3, wd=0.01):
        self.config.epochs = epochs
        self.config.lr = lr
        self.config.wd = wd
        
        self.optim = AdamW(self.model.parameters(), weight_decay=wd)
        self.loss_func = nn.CrossEntropyLoss()
        self.schedule = OneCycleLR(self.optim, max_lr=lr, 
                                   steps_per_epoch=len(self.train_dataloader), 
                                   epochs=epochs,
                                   pct_start=0.2,
                                   final_div_factor=1e3)
    
    def reset_metrics(self):
        self.train_acc.reset()
        self.valid_acc.reset()
        
    def train_step(self, loss):
        self.optim.zero_grad()
        loss.backward()
        self.optim.step()
        self.schedule.step()
        return loss
        
    def one_epoch(self, train=True, use_wandb=False):
        avg_loss = 0.
        if train: 
            self.model.train()
            dl = self.train_dataloader
        else: 
            self.model.eval()
            dl = self.valid_dataloader
        pbar = progress_bar(dl, leave=False)
        preds = []
        for i, b in enumerate(pbar):
            with torch.autocast("cuda") and (torch.inference_mode() if not train else torch.enable_grad()):
                images, labels = to_device(b, self.device)
                preds_b = self.model(images)
                loss = self.loss_func(preds_b, labels)
                avg_loss += loss
                preds.append(preds_b)
                if train:
                    self.train_step(loss)
                    acc = self.train_acc(preds_b, labels)
                    if use_wandb: 
                        wandb.log({"train_loss": loss.item(),
                                   "train_acc": acc,
                                   "learning_rate": self.schedule.get_last_lr()[0]})
                else:
                    acc = self.valid_acc(preds_b, labels)
            pbar.comment = f"train_loss={loss.item():2.3f}, train_acc={acc:2.3f}"      
            
        return torch.cat(preds, dim=0), avg_loss.mean().item()
    
    def get_model_preds(self, with_inputs=False):
        preds, loss = self.one_epoch(train=False)
        if with_inputs:
            images, labels = self.get_data_tensors()
            return images, labels, preds, loss
        else:
            return preds, loss
            
    def fit(self, use_wandb=False, log_preds=False, n_preds=None):
        if use_wandb:
            run = wandb.init(project=WANDB_PROJECT, entity=ENTITY, config=self.config)
            
        for epoch in progress_bar(range(self.config.epochs), total=self.config.epochs, leave=True):
            _  = self.one_epoch(train=True, use_wandb=use_wandb)
            
            if use_wandb:
                wandb.log({"epoch":epoch})
                
            ## validation
            if self.do_validation:
                _, avg_loss = self.one_epoch(train=False, use_wandb=use_wandb)
                if use_wandb:
                    wandb.log({"val_loss": avg_loss,
                               "val_acc": self.valid_acc.compute()})
            self.reset_metrics()
        if use_wandb:
            if log_preds:
                print("Logging model predictions on validation data")
                preds, _ = self.get_model_preds()
                self.preds_logger.log(preds=preds)
            wandb.finish()
